{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aec6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deep Learning Applications: Gradients and Information Theory\n",
    "# Week 4: Understanding Curves, Gradients, and Information Theory\n",
    "# Duration: 1 Hour\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import math\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize_scalar\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1: Understanding Curves and Derivatives (15 minutes)\n",
    "print(\"\\nüîµ PART 1: CURVES AND DERIVATIVES\")\n",
    "print(\"Understanding how functions behave and their slopes\")\n",
    "\n",
    "# Define some example functions\n",
    "def quadratic_function(x):\n",
    "    \"\"\"Simple quadratic function: f(x) = x^2 - 4x + 3\"\"\"\n",
    "    \n",
    "\n",
    "def derivative_quadratic(x):\n",
    "    \"\"\"Derivative of quadratic: f'(x) = 2x - 4\"\"\"\n",
    "    \n",
    "\n",
    "def complex_function(x):\n",
    "    \"\"\"More complex function with multiple local minima\"\"\"\n",
    "    \n",
    "\n",
    "def derivative_complex(x):\n",
    "    \"\"\"Derivative of complex function\"\"\"\n",
    "    \n",
    "\n",
    "# Visualize functions and their derivatives\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "x = np.linspace(-2, 6, 1000)\n",
    "y1 = quadratic_function(x)\n",
    "dy1 = derivative_quadratic(x)\n",
    "\n",
    "# Plot quadratic function\n",
    "ax1.plot(x, y1, 'b-', linewidth=2, label='f(x) = x¬≤ - 4x + 3')\n",
    "ax1.plot(x, dy1, 'r--', linewidth=2, label=\"f'(x) = 2x - 4\")\n",
    "ax1.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax1.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Quadratic Function and Its Derivative')\n",
    "ax1.legend()\n",
    "\n",
    "# Mark critical points\n",
    "\n",
    "\n",
    "# Plot complex function\n",
    "\n",
    "\n",
    "ax2.plot(x2, y2, 'b-', linewidth=2, label='Complex function')\n",
    "ax2.plot(x2, dy2, 'r--', linewidth=2, label='Derivative')\n",
    "ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Complex Function with Multiple Extrema')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Derivative = 0 at local minima and maxima\")\n",
    "print(\"   ‚Ä¢ Positive derivative ‚Üí function is increasing\")\n",
    "print(\"   ‚Ä¢ Negative derivative ‚Üí function is decreasing\")\n",
    "print(\"   ‚Ä¢ Steeper slope ‚Üí larger absolute derivative value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 2: Gradient Descent Algorithm (15 minutes)\n",
    "print(\"\\nüîµ PART 2: GRADIENT DESCENT\")\n",
    "print(\"Using derivatives to find minima - the heart of ML optimization!\")\n",
    "\n",
    "def gradient_descent_1d(func, grad_func, start_x, learning_rate=0.1, max_iterations=50):\n",
    "    \"\"\"\n",
    "    Simple 1D gradient descent implementation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Demonstrate gradient descent on our quadratic function\n",
    "print(\"üéØ Finding minimum of f(x) = x¬≤ - 4x + 3 using gradient descent\")\n",
    "\n",
    "start_points = [-1, 5, 0]\n",
    "colors = ['red', 'green', 'orange']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot the function\n",
    "x_plot = np.linspace(-2, 6, 1000)\n",
    "y_plot = quadratic_function(x_plot)\n",
    "ax.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = x¬≤ - 4x + 3')\n",
    "\n",
    "# Run gradient descent from different starting points\n",
    "for i, (start, color) in enumerate(zip(start_points, colors)):\n",
    "    print(f\"\\nüöÄ Starting from x = {start}\")\n",
    "    x_hist, y_hist = gradient_descent_1d(quadratic_function, derivative_quadratic, \n",
    "                                        start, learning_rate=0.3)\n",
    "    \n",
    "    # Plot the path\n",
    "    ax.plot(x_hist, y_hist, 'o-', color=color, markersize=6, \n",
    "           label=f'Path from x={start}', alpha=0.7)\n",
    "    ax.plot(start, quadratic_function(start), 's', color=color, markersize=10)\n",
    "    \n",
    "    print(f\"   üìç Final position: x = {x_hist[-1]:.4f}, y = {y_hist[-1]:.4f}\")\n",
    "\n",
    "# Mark the true minimum\n",
    "true_min_x = 2\n",
    "true_min_y = quadratic_function(true_min_x)\n",
    "ax.plot(true_min_x, true_min_y, 'k*', markersize=15, label=f'True minimum ({true_min_x}, {true_min_y})')\n",
    "\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Gradient Descent: Finding Function Minimum')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéì Learning Insights:\")\n",
    "print(\"   ‚Ä¢ Gradient descent moves opposite to the gradient direction\")\n",
    "print(\"   ‚Ä¢ Learning rate controls step size (too big = overshoot, too small = slow)\")\n",
    "print(\"   ‚Ä¢ Different starting points can lead to the same minimum\")\n",
    "print(\"   ‚Ä¢ Algorithm stops when gradient ‚âà 0 (at minimum)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 3: 2D Surfaces and Gradients (10 minutes)\n",
    "print(\"\\nüîµ PART 3: 2D SURFACES AND GRADIENTS\")\n",
    "print(\"Extending to higher dimensions - like real neural networks!\")\n",
    "\n",
    "def surface_function(x, y):\n",
    "    \"\"\"2D surface with multiple features\"\"\"\n",
    "    \n",
    "\n",
    "def gradient_2d(x, y):\n",
    "    \"\"\"Gradient of the 2D surface\"\"\"\n",
    "    \n",
    "\n",
    "# Create a grid for visualization\n",
    "x_range = np.linspace(-2, 4, 100)\n",
    "y_range = np.linspace(-2, 6, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = surface_function(X, Y)\n",
    "\n",
    "# Create 3D surface plot\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('f(x,y)')\n",
    "ax1.set_title('3D Surface')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=20)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Contour Lines')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient field\n",
    "ax3 = fig.add_subplot(133)\n",
    "x_arrows = np.linspace(-2, 4, 10)\n",
    "y_arrows = np.linspace(-2, 6, 10)\n",
    "X_arrows, Y_arrows = np.meshgrid(x_arrows, y_arrows)\n",
    "DX, DY = gradient_2d(X_arrows, Y_arrows)\n",
    "\n",
    "# Plot gradient vectors\n",
    "ax3.quiver(X_arrows, Y_arrows, -DX, -DY, alpha=0.7, color='red', \n",
    "          label='Negative gradient (descent direction)')\n",
    "ax3.contour(X, Y, Z, levels=10, alpha=0.3)\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title('Gradient Field')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark minimum\n",
    "min_x, min_y = 1, 2  # analytical minimum\n",
    "ax3.plot(min_x, min_y, 'go', markersize=12, label=f'Minimum ({min_x}, {min_y})')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Key Concepts:\")\n",
    "print(\"   ‚Ä¢ Gradient vectors point toward steepest ascent\")\n",
    "print(\"   ‚Ä¢ Negative gradient points toward steepest descent\")\n",
    "print(\"   ‚Ä¢ Contour lines connect points of equal function value\")\n",
    "print(\"   ‚Ä¢ At minimum/maximum, gradient = zero vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f68122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 4: Information Theory Basics (20 minutes)\n",
    "print(\"\\nüîµ PART 4: INFORMATION THEORY\")\n",
    "print(\"From Claude Shannon to modern deep learning!\")\n",
    "\n",
    "# Sample text for analysis\n",
    "sample_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This pangram contains every letter \n",
    "of the alphabet at least once. Information theory helps us understand how to \n",
    "compress and transmit messages efficiently. The more predictable a message, \n",
    "the less information it contains. Surprising events carry more information.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"Calculate entropy of a text string\"\"\"\n",
    "    # Count character frequencies\n",
    "\n",
    "\n",
    "# Analyze our sample text\n",
    "entropy, char_counts = calculate_entropy(sample_text)\n",
    "\n",
    "print(f\"üìä Text Analysis:\")\n",
    "print(f\"   ‚Ä¢ Total characters: {len(sample_text)}\")\n",
    "print(f\"   ‚Ä¢ Unique characters: {len(char_counts)}\")\n",
    "print(f\"   ‚Ä¢ Entropy: {entropy:.3f} bits per character\")\n",
    "print(f\"   ‚Ä¢ Max possible entropy: {math.log2(len(char_counts)):.3f} bits\")\n",
    "\n",
    "# Visualize character frequencies\n",
    "most_common = char_counts.most_common(15)\n",
    "chars, counts = zip(*most_common)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Character frequency plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(chars)), counts)\n",
    "plt.xticks(range(len(chars)), [f\"'{c}'\" if c != ' ' else \"'space'\" for c in chars], rotation=45)\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Character Frequency Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Information content plot (surprise)\n",
    "probabilities = [count/len(sample_text) for count in counts]\n",
    "information_content = [-math.log2(p) for p in probabilities]\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(chars)), information_content, color='orange')\n",
    "plt.xticks(range(len(chars)), [f\"'{c}'\" if c != ' ' else \"'space'\" for c in chars], rotation=45)\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Information Content (bits)')\n",
    "plt.title('Information Content per Character')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Information Theory Insights:\")\n",
    "print(\"   ‚Ä¢ Common characters (like 'e', space) have low information content\")\n",
    "print(\"   ‚Ä¢ Rare characters carry more information (higher surprise)\")\n",
    "print(\"   ‚Ä¢ Entropy measures average information content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Example\n",
    "print(\"\\nüîµ CROSS ENTROPY EXAMPLE\")\n",
    "print(\"Comparing different probability distributions\")\n",
    "\n",
    "def cross_entropy(p_true, p_predicted):\n",
    "    \"\"\"Calculate cross entropy between true and predicted distributions\"\"\"\n",
    "    # Avoid log(0) by adding small epsilon\n",
    "\n",
    "\n",
    "# Example: Predicting next character probabilities\n",
    "print(\"üéØ Scenario: Predicting next character after 'th'\")\n",
    "\n",
    "# True distribution (what actually happens after 'th')\n",
    "true_chars = ['e', 'a', 'i', 'o', 'u', 'other']\n",
    "true_probs = np.array([0.6, 0.15, 0.1, 0.05, 0.05, 0.05])  # 'e' is most common after 'th'\n",
    "\n",
    "# Different prediction models\n",
    "\n",
    "\n",
    "print(f\"{'Model':<15} {'Cross Entropy':<15} {'Performance'}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for model_name, predicted_probs in models.items():\n",
    "    ce = cross_entropy(true_probs, predicted_probs)\n",
    "    if ce < 2:\n",
    "        performance = \"Excellent\"\n",
    "    elif ce < 3:\n",
    "        performance = \"Good\"\n",
    "    elif ce < 4:\n",
    "        performance = \"Poor\"\n",
    "    else:\n",
    "        performance = \"Very Poor\"\n",
    "    \n",
    "    print(f\"{model_name:<15} {ce:<15.3f} {performance}\")\n",
    "\n",
    "# Visualize the distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "width = 0.35\n",
    "x_pos = np.arange(len(true_chars))\n",
    "\n",
    "for i, (model_name, predicted_probs) in enumerate(models.items()):\n",
    "    ax = axes[i]\n",
    "    ax.bar(x_pos - width/2, true_probs, width, label='True Distribution', alpha=0.7)\n",
    "    ax.bar(x_pos + width/2, predicted_probs, width, label='Predicted Distribution', alpha=0.7)\n",
    "    ax.set_xlabel('Characters after \"th\"')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'{model_name}')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(true_chars)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12caf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
